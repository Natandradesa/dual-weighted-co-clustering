{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from algorithms.state_of_the_art_algorithms import FuzzyDoubleKmeans, WFDK\n",
    "from algorithms.kernel_fuzzy_coclustering import GKFDK, WGKFDK\n",
    "from algorithms.double_subspace_coclustering import DWGKFDK\n",
    "from sklearn.metrics  import adjusted_rand_score\n",
    "from metrics import frigui_index, fuzzy_to_crisp\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import pickle\n",
    "import copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def get_metrics(y, U):\n",
    "    y_hat = fuzzy_to_crisp(U)\n",
    "    metrics = np.full(2,np.nan)\n",
    "    metrics[0] = adjusted_rand_score(y, y_hat)\n",
    "    metrics[1] = frigui_index(U,y)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def apply_N_times(X, model, y , n = 100, path = None, random_state = 26):\n",
    "    rs = pd.DataFrame(np.zeros((n,2)))\n",
    "    for i in range(n):\n",
    "        mdl = copy.deepcopy(model)\n",
    "        seed = random_state + n + i\n",
    "        fitted_model = mdl.fit(X = X, random_state = seed) # training step\n",
    "        rs.loc[i,:] = get_metrics(y=y, U=fitted_model.U)\n",
    "        seed = seed + 1  \n",
    "        print(f\"Iteração: {i+1}/{n}\", end='\\r')\n",
    "    rs.columns = [ 'ARI', 'FRG']\n",
    "    if path is not None:\n",
    "        rs.to_csv(path + '.txt', header= True, index = False)\n",
    "    else:\n",
    "        return rs\n",
    "\n",
    "def standardization(X):\n",
    "    norm = lambda x: (x - np.mean(x))/(np.std(x))\n",
    "    if type(X) == np.ndarray:\n",
    "        X = pd.DataFrame(X)\n",
    "    X_scale = X.apply(norm, axis=0)\n",
    "    X_scale = X_scale.fillna(0)\n",
    "    return X_scale\n",
    "\n",
    "def numerical_class(y):\n",
    "    return np.unique(y, return_inverse=True)[1]\n",
    "\n",
    "def load_datasets(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        info = pickle.load(f)\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "dtsets = ['wdbc', 'vehicle', 'chemical_composition', 'vertebral_column_2C', 'vertebral_column', 'breast_tissue', 'abalone', 'fruit', 'gtzan', 'tox_171']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = np.zeros((len(dtsets), 3))\n",
    "for i in range(len(dtsets)):\n",
    "    path = 'datasets/' + dtsets[i]\n",
    "    X, y,_ = load_datasets(path)\n",
    "    N,P = X.shape\n",
    "    K = len(np.unique(y))\n",
    "    rs[i] = np.array([N,P,K])\n",
    "rs = rs.astype('int')\n",
    "pd.DataFrame(rs, index = dtsets, columns=['N','P','K'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameters = pd.read_csv('configurations/FDK.csv', index_col=0)\n",
    "#path_final = 'Resultados/real_data'\n",
    "path_final = 'results_real'\n",
    "model = 'fdk'\n",
    "for i in range(len(dtsets)):\n",
    "    dt = dtsets[i]\n",
    "    path1 = path_final + '/' + dt\n",
    "    if not os.path.exists(path1):\n",
    "        os.makedirs(path1)\n",
    "        \n",
    "    path2 = path1 + '/' + model\n",
    "    m = best_hyperparameters.loc[dt,'m']\n",
    "    n = best_hyperparameters.loc[dt,'n']\n",
    "    X, y,_ = load_datasets('datasets/' + dt)\n",
    "    X = standardization(X)\n",
    "    y = numerical_class(y)\n",
    "    n_clusters = len(np.unique(y))\n",
    "    mdl = FuzzyDoubleKmeans(K = n_clusters, H = n_clusters, m = m, n = n)\n",
    "    print(f\"Dataset: {dt}(K = H = {n_clusters})\")\n",
    "    apply_N_times(X = X, model = mdl, y = y, n = 100, path = path2 , random_state = 100)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WFDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameters = pd.read_csv('configurations/WFDK.csv', index_col=0)\n",
    "path_final = 'results_real'\n",
    "model = 'wfdk'\n",
    "for i in range(len(dtsets)):\n",
    "    dt = dtsets[i]\n",
    "    path1 = path_final + '/' + dt\n",
    "    if not os.path.exists(path1):\n",
    "        os.makedirs(path1)\n",
    "        \n",
    "    path2 = path1 + '/' + model\n",
    "    m = best_hyperparameters.loc[dt,'m']\n",
    "    n = best_hyperparameters.loc[dt,'n']\n",
    "    gamma = best_hyperparameters.loc[dt,'gamma']\n",
    "    X, y,_ = load_datasets('datasets/' + dt)\n",
    "    X = standardization(X)\n",
    "    y = numerical_class(y)\n",
    "    n_clusters = len(np.unique(y))\n",
    "    mdl = WFDK(K = n_clusters, H = n_clusters, m = m, n = n, gamma = gamma)\n",
    "    print(f\"Dataset: {dt}(K = H = {n_clusters})\")\n",
    "    apply_N_times(X = X, model = mdl, y = y, n = 100, path = path2 , random_state = 100)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GKFDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caputo = pd.read_csv('configurations/caputo.csv', index_col=0)\n",
    "best_hyperparameters = pd.read_csv('configurations/GKFDK.csv', index_col=0)\n",
    "path_final = 'results_real'\n",
    "model = 'gkfdk'\n",
    "for i in range(len(dtsets)):\n",
    "    dt = dtsets[i]\n",
    "    path1 = path_final + '/' + dt\n",
    "    if not os.path.exists(path1):\n",
    "        os.makedirs(path1)\n",
    "        \n",
    "    path2 = path1 + '/' + model\n",
    "    sig2 = caputo.loc[dt,'sigma2']\n",
    "    m = best_hyperparameters.loc[dt,'m']\n",
    "    n = best_hyperparameters.loc[dt,'n']\n",
    "    X, y,_ = load_datasets('datasets/' + dt)\n",
    "    X = standardization(X)\n",
    "    y = numerical_class(y)\n",
    "    n_clusters = len(np.unique(y))\n",
    "    mdl = GKFDK(K = n_clusters, H = n_clusters, m = m, n = n,sigma2 = sig2, epsilon = 1e-5)\n",
    "    print(f\"Dataset: {dt}(K = H = {n_clusters})\")\n",
    "    apply_N_times(X = X, model = mdl, y = y, n = 100, path = path2 , random_state = 100)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WGKFDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caputo = pd.read_csv('configurations/caputo.csv', index_col=0)\n",
    "best_hyperparameters = pd.read_csv('configurations/WGKFDK.csv', index_col=0)\n",
    "path_final = 'results_real'\n",
    "model = 'wgkfdk'\n",
    "for i in range(len(dtsets)):\n",
    "    dt = dtsets[i]\n",
    "    path1 = path_final + '/' + dt\n",
    "    if not os.path.exists(path1):\n",
    "        os.makedirs(path1)\n",
    "        \n",
    "    path2 = path1 + '/' + model\n",
    "    sig2 = caputo.loc[dt,'sigma2']\n",
    "    m = best_hyperparameters.loc[dt,'m']\n",
    "    n = best_hyperparameters.loc[dt,'n']\n",
    "    X, y,_ = load_datasets('datasets/' + dt)\n",
    "    X = standardization(X)\n",
    "    y = numerical_class(y)\n",
    "    n_clusters = len(np.unique(y))\n",
    "    mdl = WGKFDK(K = n_clusters, H = n_clusters, m = m, n = n,sigma2 = sig2, epsilon = 1e-5)\n",
    "    print(f\"Dataset: {dt}(K = H = {n_clusters})\")\n",
    "    apply_N_times(X = X, model = mdl, y = y, n = 100, path = path2 , random_state = 100)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DWGKFDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caputo = pd.read_csv('configurations/caputo.csv', index_col=0)\n",
    "best_hyperparameters = pd.read_csv('configurations/DWGKFDK.csv', index_col=0)\n",
    "path_final = 'results_real'\n",
    "model = 'dwgkfdk'\n",
    "for i in range(len(dtsets)):\n",
    "    dt = dtsets[i]\n",
    "    path1 = path_final + '/' + dt\n",
    "    if not os.path.exists(path1):\n",
    "        os.makedirs(path1)\n",
    "        \n",
    "    path2 = path1 + '/' + model\n",
    "    sig2 = caputo.loc[dt,'sigma2']\n",
    "    m = best_hyperparameters.loc[dt,'m']\n",
    "    n = best_hyperparameters.loc[dt,'n']\n",
    "    X, y,_ = load_datasets('datasets/' + dt)\n",
    "    X = standardization(X)\n",
    "    y = numerical_class(y)\n",
    "    n_clusters = len(np.unique(y))\n",
    "    mdl = DWGKFDK(K = n_clusters, H = n_clusters, m = m, n = n,sigma2 = sig2, epsilon = 1e-5)\n",
    "    print(f\"Dataset: {dt}(K = H = {n_clusters})\")\n",
    "    apply_N_times(X = X, model = mdl, y = y, n = 100, path = path2 , random_state = 100)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
